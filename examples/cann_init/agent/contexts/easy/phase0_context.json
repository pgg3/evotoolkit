{
  "op_name": "Relu",
  "signature": {
    "op_name": "Relu",
    "inputs": [
      {
        "name": "x",
        "dtype": "float",
        "is_tensor": true
      }
    ],
    "outputs": [
      {
        "name": "output",
        "dtype": "float",
        "is_tensor": true
      }
    ],
    "init_params": []
  },
  "shape_inference": {
    "input": "[*] (any shape)",
    "output": "same as input",
    "formula": "auto output_shape = x.sizes();"
  },
  "functionality": "Applies ReLU activation max(0, x) element-wise to the input tensor, replacing negative values with zero.",
  "strategies": {
    "kernel": "generate",
    "tiling": "default",
    "pybind": "default"
  }
}