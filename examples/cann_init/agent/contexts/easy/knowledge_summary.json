{
  "api_summaries": [
    {
      "name": "Relu",
      "signature": "(1) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)\n(2) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)\n(3) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)",
      "description": "dst[i] = (src[i] < 0) ? 0 : src[i]",
      "category": "vec_unary",
      "header": "kernel_operator_vec_unary_intf.h"
    },
    {
      "name": "DataCopy",
      "signature": "(1) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal, const Nd2NzParams& intriParams)\n(2) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcGlobal, const Nd2NzParams& intriParams)\n(3) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal, const DataCopyParams& repeatParams)\n(4) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const DataCopyParams& repeatParams)\n(5) void DataCopy(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src_T>& srcLocal, const DataCopyParams& repeatParams)\n... (共 17 个重载)",
      "description": "format transform(such as nd2nz) during data load from OUT to L1",
      "category": "data_copy",
      "header": "kernel_operator_data_copy_intf.h"
    },
    {
      "name": "GetBlockIdx",
      "signature": "int64_t GetBlockIdx()",
      "description": "",
      "category": "common",
      "header": "kernel_operator_common_intf.h"
    }
  ],
  "example_summaries": [
    {
      "name": "squared_relu",
      "purpose": "This operator uses Relu as part of its computation pipeline, demonstrating the exact API usage, multi-core tiling, ping-pong buffering, and data type handling patterns needed for implementing Relu.",
      "adaptation_notes": "- Remove the Mul operation (keep only Relu)\n- Simplify Compute to only call Relu without squaring\n- Keep the same multi-core distribution logic, ping-pong buffer management, and Cast operations for different data types\n- Maintain the same DataCopyPad and event synchronization patterns",
      "init_pattern": "__aicore__ inline void SquaredReluND<T>::Init(GM_ADDR input, GM_ADDR output, GM_ADDR workspace,\n                                               const SquaredReluTilingData* tilingData) {\n    inputGm.SetGlobalBuffer((__gm__ T*)input);\n    outputGm.SetGlobalBuffer((__gm__ T*)output);\n    elementNum = tilingData->elementNum;\n    needCoreNumber = tilingData->needCoreNum;\n    blockIdx = GetBlockIdx();\n    pipe.InitBuffer(ubTBuf, MAX_UB_SIZE);\n    tmpTensor = ubTBuf.Get<uint8_t>();\n}",
      "process_pattern": "__aicore__ inline void SquaredReluND<T>::Process() {\n    if (blockIdx >= needCoreNumber) {\n        return;\n    }\n    int64_t totalTimes = elementNum / PP_ELEMENT_NUM;\n    int64_t remain = elementNum % PP_ELEMENT_NUM;\n    if (remain > 0) {\n        totalTimes++;\n    }\n    int64_t loopNum = totalTimes / needCoreNumber;\n    int64_t loopRemain = totalTimes % needCoreNumber;\n    if (loopRemain > 0 && blockIdx < loopRemain) {\n        loopNum++;\n    }\n    int64_t eachCoreStartOffset = loopNum * blockIdx * PP_ELEMENT_NUM;\n    if (loopRemain > 0) {\n        if (blockIdx >= loopRemain) {\n            eachCoreStartOffset += elementNum % (PP_ELEMENT_NUM * needCoreNumber);\n        }\n    }\n    int32_t calNum = PP_ELEMENT_NUM;\n    int64_t lastCoreNum = loopRemain == 0 ? needCoreNumber - 1 : loopRemain - 1;\n    pingPongFlag = 0;\n    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\n    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);\n    for (int64_t i = 0; i < loopNum; i++) {\n        int64_t localOffset = i * PP_ELEMENT_NUM;\n        if (remain > 0 && i == loopNum -1 && blockIdx == lastCoreNum) {\n            calNum = remain;\n        }\n        eventId = pingPongFlag ? EVENT_ID1 : EVENT_ID0;\n        CopyInAndCast(eachCoreStartOffset + localOffset, calNum);\n        Compute(calNum);\n        CastAndCopyOut(eachCoreStartOffset + localOffset, calNum);\n        pingPongFlag = 1 - pingPongFlag;\n    }\n    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\n    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);\n}",
      "compute_pattern": "__aicore__ inline void SquaredReluND<T>::Compute(int64_t dataCount) {\n    Relu(xTensorFp32, xTensorFp32, dataCount);\n    PipeBarrier<PIPE_V>();\n    Mul(xTensorFp32, xTensorFp32, xTensorFp32, dataCount);\n    PipeBarrier<PIPE_V>();\n}",
      "copyinout_pattern": "// CopyInAndCast\n__aicore__ inline void SquaredReluND<T>::CopyInAndCast(int64_t inputOffset, int64_t dataCount) {\n    xTensor = pingPongFlag ?\n            tmpTensor[MAX_UB_SIZE / 2].ReinterpretCast<T>() :\n            tmpTensor[0].ReinterpretCast<T>();\n    WaitFlag<HardEvent::MTE3_MTE2>(eventId);\n    DataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\n    DataCopyPadExtParams<T> padParams{false, 0, 0, 0};\n    if (std::is_same_v<T, bfloat16_t> || std::is_same_v<T, half>) {\n        int32_t elementByte = PP_ELEMENT_NUM * sizeof(T);\n        xTmp = pingPongFlag ?\n                tmpTensor[elementByte + MAX_UB_SIZE / 2].ReinterpretCast<T>() :\n                tmpTensor[elementByte].ReinterpretCast<T>();\n        DataCopyPad(xTmp, inputGm[inputOffset], dataCopyParams, padParams);\n    } else {\n        DataCopyPad(xTensor, inputGm[inputOffset], dataCopyParams, padParams);\n    }\n    SetFlag<HardEvent::MTE2_V>(eventId);\n    WaitFlag<HardEvent::MTE2_V>(eventId);\n    xTensorFp32 = xTensor.template ReinterpretCast<float>();\n    if (std::is_same_v<T, bfloat16_t> || std::is_same_v<T, half>) {\n        Cast(xTensorFp32, xTmp, RoundMode::CAST_NONE, dataCount);\n        PipeBarrier<PIPE_V>();\n    }\n}\n\n// CastAndCopyOut\n__aicore__ inline void SquaredReluND<T>::CastAndCopyOut(int64_t outputOffset, int64_t dataCount) {\n    if (std::is_same_v<T, half>) {\n        Cast(xTensor, xTensorFp32, RoundMode::CAST_NONE, dataCount);\n        PipeBarrier<PIPE_V>();\n    } else if (std::is_same_v<T, bfloat16_t>) {\n        Cast(xTensor, xTensorFp32, RoundMode::CAST_RINT, dataCount);\n        PipeBarrier<PIPE_V>();\n    }\n    SetFlag<HardEvent::V_MTE3>(eventId);\n    WaitFlag<HardEvent::V_MTE3>(eventId);\n    DataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\n    DataCopyPad(outputGm[outputOffset], xTensor, dataCopyParams);\n}",
      "key_api_calls": "inputGm.SetGlobalBuffer((__gm__ T*)input);\noutputGm.SetGlobalBuffer((__gm__ T*)output);\npipe.InitBuffer(ubTBuf, MAX_UB_SIZE);\ntmpTensor = ubTBuf.Get<uint8_t>();\nGetBlockIdx();\nxTensor = tmpTensor[offset].ReinterpretCast<T>();\nxTensorFp32 = xTensor.template ReinterpretCast<float>();\nDataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\nDataCopyPadExtParams<T> padParams{false, 0, 0, 0};\nDataCopyPad(dst, src, dataCopyParams, padParams);\nDataCopyPad(dst, src, dataCopyParams);\nCast(xTensorFp32, xTmp, RoundMode::CAST_NONE, dataCount);\nCast(xTensor, xTensorFp32, RoundMode::CAST_RINT, dataCount);\nRelu(xTensorFp32, xTensorFp32, dataCount);\nPipeBarrier<PIPE_V>();\nSetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\nSetFlag<HardEvent::MTE2_V>(eventId);\nSetFlag<HardEvent::V_MTE3>(eventId);\nWaitFlag<HardEvent::MTE3_MTE2>(eventId);\nWaitFlag<HardEvent::MTE2_V>(eventId);\nWaitFlag<HardEvent::V_MTE3>(eventId);",
      "kernel_snippet": "",
      "tiling_snippet": ""
    }
  ],
  "combined_context": "## API Reference\n\n### Relu\n- **Signature**: `(1) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)\n(2) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)\n(3) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)`\n- **Description**: dst[i] = (src[i] < 0) ? 0 : src[i]\n\n### DataCopy\n- **Signature**: `(1) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal, const Nd2NzParams& intriParams)\n(2) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcGlobal, const Nd2NzParams& intriParams)\n(3) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal, const DataCopyParams& repeatParams)\n(4) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const DataCopyParams& repeatParams)\n(5) void DataCopy(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src_T>& srcLocal, const DataCopyParams& repeatParams)\n... (共 17 个重载)`\n- **Description**: format transform(such as nd2nz) during data load from OUT to L1\n\n### GetBlockIdx\n- **Signature**: `int64_t GetBlockIdx()`\n\n## Example Reference\n\n### squared_relu\n**Why Selected**: This operator uses Relu as part of its computation pipeline, demonstrating the exact API usage, multi-core tiling, ping-pong buffering, and data type handling patterns needed for implementing Relu.\n\n**Adaptation Notes**: - Remove the Mul operation (keep only Relu)\n- Simplify Compute to only call Relu without squaring\n- Keep the same multi-core distribution logic, ping-pong buffer management, and Cast operations for different data types\n- Maintain the same DataCopyPad and event synchronization patterns\n\n**Init Pattern** (buffer setup):\n```cpp\n__aicore__ inline void SquaredReluND<T>::Init(GM_ADDR input, GM_ADDR output, GM_ADDR workspace,\n                                               const SquaredReluTilingData* tilingData) {\n    inputGm.SetGlobalBuffer((__gm__ T*)input);\n    outputGm.SetGlobalBuffer((__gm__ T*)output);\n    elementNum = tilingData->elementNum;\n    needCoreNumber = tilingData->needCoreNum;\n    blockIdx = GetBlockIdx();\n    pipe.InitBuffer(ubTBuf, MAX_UB_SIZE);\n    tmpTensor = ubTBuf.Get<uint8_t>();\n}\n```\n\n**Process Pattern** (main loop):\n```cpp\n__aicore__ inline void SquaredReluND<T>::Process() {\n    if (blockIdx >= needCoreNumber) {\n        return;\n    }\n    int64_t totalTimes = elementNum / PP_ELEMENT_NUM;\n    int64_t remain = elementNum % PP_ELEMENT_NUM;\n    if (remain > 0) {\n        totalTimes++;\n    }\n    int64_t loopNum = totalTimes / needCoreNumber;\n    int64_t loopRemain = totalTimes % needCoreNumber;\n    if (loopRemain > 0 && blockIdx < loopRemain) {\n        loopNum++;\n    }\n    int64_t eachCoreStartOffset = loopNum * blockIdx * PP_ELEMENT_NUM;\n    if (loopRemain > 0) {\n        if (blockIdx >= loopRemain) {\n            eachCoreStartOffset += elementNum % (PP_ELEMENT_NUM * needCoreNumber);\n        }\n    }\n    int32_t calNum = PP_ELEMENT_NUM;\n    int64_t lastCoreNum = loopRemain == 0 ? needCoreNumber - 1 : loopRemain - 1;\n    pingPongFlag = 0;\n    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\n    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);\n    for (int64_t i = 0; i < loopNum; i++) {\n        int64_t localOffset = i * PP_ELEMENT_NUM;\n        if (remain > 0 && i == loopNum -1 && blockIdx == lastCoreNum) {\n            calNum = remain;\n        }\n        eventId = pingPongFlag ? EVENT_ID1 : EVENT_ID0;\n        CopyInAndCast(eachCoreStartOffset + localOffset, calNum);\n        Compute(calNum);\n        CastAndCopyOut(eachCoreStartOffset + localOffset, calNum);\n        pingPongFlag = 1 - pingPongFlag;\n    }\n    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\n    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);\n}\n```\n\n**Compute Pattern** (core computation):\n```cpp\n__aicore__ inline void SquaredReluND<T>::Compute(int64_t dataCount) {\n    Relu(xTensorFp32, xTensorFp32, dataCount);\n    PipeBarrier<PIPE_V>();\n    Mul(xTensorFp32, xTensorFp32, xTensorFp32, dataCount);\n    PipeBarrier<PIPE_V>();\n}\n```\n\n**CopyIn/CopyOut Pattern** (data movement):\n```cpp\n// CopyInAndCast\n__aicore__ inline void SquaredReluND<T>::CopyInAndCast(int64_t inputOffset, int64_t dataCount) {\n    xTensor = pingPongFlag ?\n            tmpTensor[MAX_UB_SIZE / 2].ReinterpretCast<T>() :\n            tmpTensor[0].ReinterpretCast<T>();\n    WaitFlag<HardEvent::MTE3_MTE2>(eventId);\n    DataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\n    DataCopyPadExtParams<T> padParams{false, 0, 0, 0};\n    if (std::is_same_v<T, bfloat16_t> || std::is_same_v<T, half>) {\n        int32_t elementByte = PP_ELEMENT_NUM * sizeof(T);\n        xTmp = pingPongFlag ?\n                tmpTensor[elementByte + MAX_UB_SIZE / 2].ReinterpretCast<T>() :\n                tmpTensor[elementByte].ReinterpretCast<T>();\n        DataCopyPad(xTmp, inputGm[inputOffset], dataCopyParams, padParams);\n    } else {\n        DataCopyPad(xTensor, inputGm[inputOffset], dataCopyParams, padParams);\n    }\n    SetFlag<HardEvent::MTE2_V>(eventId);\n    WaitFlag<HardEvent::MTE2_V>(eventId);\n    xTensorFp32 = xTensor.template ReinterpretCast<float>();\n    if (std::is_same_v<T, bfloat16_t> || std::is_same_v<T, half>) {\n        Cast(xTensorFp32, xTmp, RoundMode::CAST_NONE, dataCount);\n        PipeBarrier<PIPE_V>();\n    }\n}\n\n// CastAndCopyOut\n__aicore__ inline void SquaredReluND<T>::CastAndCopyOut(int64_t outputOffset, int64_t dataCount) {\n    if (std::is_same_v<T, half>) {\n        Cast(xTensor, xTensorFp32, RoundMode::CAST_NONE, dataCount);\n        PipeBarrier<PIPE_V>();\n    } else if (std::is_same_v<T, bfloat16_t>) {\n        Cast(xTensor, xTensorFp32, RoundMode::CAST_RINT, dataCount);\n        PipeBarrier<PIPE_V>();\n    }\n    SetFlag<HardEvent::V_MTE3>(eventId);\n    WaitFlag<HardEvent::V_MTE3>(eventId);\n    DataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\n    DataCopyPad(outputGm[outputOffset], xTensor, dataCopyParams);\n}\n```\n\n**Key API Calls** (use these exact patterns):\n```cpp\ninputGm.SetGlobalBuffer((__gm__ T*)input);\noutputGm.SetGlobalBuffer((__gm__ T*)output);\npipe.InitBuffer(ubTBuf, MAX_UB_SIZE);\ntmpTensor = ubTBuf.Get<uint8_t>();\nGetBlockIdx();\nxTensor = tmpTensor[offset].ReinterpretCast<T>();\nxTensorFp32 = xTensor.template ReinterpretCast<float>();\nDataCopyExtParams dataCopyParams{1, static_cast<uint32_t>(dataCount * sizeof(T)), 0, 0, 0};\nDataCopyPadExtParams<T> padParams{false, 0, 0, 0};\nDataCopyPad(dst, src, dataCopyParams, padParams);\nDataCopyPad(dst, src, dataCopyParams);\nCast(xTensorFp32, xTmp, RoundMode::CAST_NONE, dataCount);\nCast(xTensor, xTensorFp32, RoundMode::CAST_RINT, dataCount);\nRelu(xTensorFp32, xTensorFp32, dataCount);\nPipeBarrier<PIPE_V>();\nSetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);\nSetFlag<HardEvent::MTE2_V>(eventId);\nSetFlag<HardEvent::V_MTE3>(eventId);\nWaitFlag<HardEvent::MTE3_MTE2>(eventId);\nWaitFlag<HardEvent::MTE2_V>(eventId);\nWaitFlag<HardEvent::V_MTE3>(eventId);\n```\n\n"
}